{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE in TensorFlow\n",
    "\n",
    "Just like we did before for q-learning, this time we'll design a neural network to learn `CartPole-v0` via policy gradient (REINFORCE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting virtual X frame buffer: Xvfb.\r\n"
     ]
    }
   ],
   "source": [
    "# This code creates a virtual display to draw game images on. \n",
    "# If you are running locally, just ignore it\n",
    "\n",
    "import os\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
    "    !bash ../xvfb start\n",
    "    os.environ['DISPLAY'] = ':1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f741442afd0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmf8Dk+bpixZkuSoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "# gym compatibility: unwrap TimeLimit\n",
    "if hasattr(env,'env'):\n",
    "    env=env.env\n",
    "\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape\n",
    "\n",
    "plt.imshow(env.render(\"rgb_array\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the policy network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For REINFORCE algorithm, we'll need a model that predicts action probabilities given states.\n",
    "\n",
    "For numerical stability, please __do not include the softmax layer into your network architecture__. \n",
    "\n",
    "We'll use softmax or log-softmax where appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# create input variables. We only need <s,a,R> for REINFORCE\n",
    "states = tf.placeholder('float32', (None,)+state_dim, name=\"states\")\n",
    "actions = tf.placeholder('int32', name=\"action_ids\")\n",
    "cumulative_rewards = tf.placeholder('float32', name=\"cumulative_returns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import InputLayer, Dense\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(InputLayer(state_dim))\n",
    "model.add(Dense(48, activation='tanh'))\n",
    "model.add(Dense(48, activation='tanh'))\n",
    "model.add(Dense(48, activation='tanh'))\n",
    "model.add(Dense(n_actions))\n",
    "\n",
    "logits = model(states)\n",
    "\n",
    "policy = tf.nn.softmax(logits)\n",
    "log_policy = tf.nn.log_softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# utility function to pick action in one given state\n",
    "def get_action_proba(s): \n",
    "    return policy.eval({states: [s]})[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function and updates\n",
    "\n",
    "We now need to define objective and update over policy gradient.\n",
    "\n",
    "Our objective function is\n",
    "\n",
    "$$ J \\approx  { 1 \\over N } \\sum  _{s_i,a_i} \\pi_\\theta (a_i | s_i) \\cdot G(s_i,a_i) $$\n",
    "\n",
    "\n",
    "Following the REINFORCE algorithm, we can define our objective as follows: \n",
    "\n",
    "$$ \\hat J \\approx { 1 \\over N } \\sum  _{s_i,a_i} log \\pi_\\theta (a_i | s_i) \\cdot G(s_i,a_i) $$\n",
    "\n",
    "When you compute gradient of that function over network weights $ \\theta $, it will become exactly the policy gradient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# select log-probabilities for chosen actions, log pi(a_i|s_i)\n",
    "indices = tf.stack([tf.range(tf.shape(log_policy)[0]), actions], axis=-1)\n",
    "log_policy_for_actions = tf.gather_nd(log_policy, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# REINFORCE objective function\n",
    "# hint: you need to use log_policy_for_actions to get log probabilities for actions taken\n",
    "\n",
    "# J =  # <policy objective as in the last formula. Please use mean, not sum.>\n",
    "J = tf.reduce_mean(tf.multiply(log_policy_for_actions, cumulative_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# regularize with entropy\n",
    "entropy = -tf.reduce_mean(tf.multiply(policy, log_policy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# all network weights\n",
    "all_weights = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "\n",
    "# weight updates. maximizing J is same as minimizing -J. Adding negative entropy.\n",
    "loss = -J - 0.1*entropy\n",
    "\n",
    "update = tf.train.AdamOptimizer().minimize(loss, var_list=all_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing cumulative rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cumulative_rewards(rewards,    # rewards at each step\n",
    "                           gamma=0.99  # discount for reward\n",
    "                           ):\n",
    "    \"\"\"\n",
    "    take a list of immediate rewards r(s,a) for the whole session \n",
    "    compute cumulative rewards R(s,a) (a.k.a. G(s,a) in Sutton '16)\n",
    "    R_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...\n",
    "\n",
    "    The simple way to compute cumulative rewards is to iterate from last to first time tick\n",
    "    and compute R_t = r_t + gamma*R_{t+1} recurrently\n",
    "\n",
    "    You must return an array/list of cumulative rewards with as many elements as in the initial rewards.\n",
    "    \"\"\"\n",
    "    l = lambda arr, g: sum([j if i==0 else g**i * j for i, j in enumerate(arr)])\n",
    "    cumm_rewards = []\n",
    "    \n",
    "    for i in range(len(rewards)):\n",
    "        cumm_rewards.append(l(rewards[i:], gamma))\n",
    "\n",
    "    return cumm_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "looks good!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "assert len(get_cumulative_rewards(range(100))) == 100\n",
    "assert np.allclose(get_cumulative_rewards([0, 0, 1, 0, 0, 1, 0], gamma=0.9),\n",
    "                   [1.40049, 1.5561, 1.729, 0.81, 0.9, 1.0, 0.0])\n",
    "assert np.allclose(get_cumulative_rewards([0, 0, 1, -2, 3, -4, 0], gamma=0.5),\n",
    "                   [0.0625, 0.125, 0.25, -1.5, 1.0, -4.0, 0.0])\n",
    "assert np.allclose(get_cumulative_rewards([0, 0, 1, 2, 3, 4, 0], gamma=0),\n",
    "                   [0, 0, 1, 2, 3, 4, 0])\n",
    "print(\"looks good!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_step(_states, _actions, _rewards):\n",
    "    \"\"\"given full session, trains agent with policy gradient\"\"\"\n",
    "    _cumulative_rewards = get_cumulative_rewards(_rewards)\n",
    "    update.run({states: _states, actions: _actions,\n",
    "                cumulative_rewards: _cumulative_rewards})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_session(t_max=1000):\n",
    "    \"\"\"play env with REINFORCE agent and train at the session end\"\"\"\n",
    "\n",
    "    # arrays to record session\n",
    "    states, actions, rewards = [], [], []\n",
    "\n",
    "    s = env.reset()\n",
    "\n",
    "    for t in range(t_max):\n",
    "\n",
    "        # action probabilities array aka pi(a|s)\n",
    "        action_probas = get_action_proba(s)\n",
    "\n",
    "        a = np.random.choice(n_actions, p=action_probas)\n",
    "\n",
    "        new_s, r, done, info = env.step(a)\n",
    "\n",
    "        # record session history to train later\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        rewards.append(r)\n",
    "\n",
    "        s = new_s\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    train_step(states, actions, rewards)\n",
    "\n",
    "    # technical: return session rewards to print them later\n",
    "    return sum(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward:71.630\n",
      "mean reward:181.230\n",
      "mean reward:199.910\n",
      "mean reward:200.000\n",
      "mean reward:199.870\n",
      "mean reward:189.740\n",
      "mean reward:189.490\n",
      "mean reward:199.870\n",
      "mean reward:156.170\n",
      "mean reward:185.290\n",
      "mean reward:142.580\n",
      "mean reward:162.080\n",
      "mean reward:169.640\n",
      "mean reward:188.480\n",
      "mean reward:148.230\n",
      "mean reward:150.610\n",
      "mean reward:135.760\n",
      "mean reward:121.290\n",
      "mean reward:120.540\n",
      "mean reward:199.550\n",
      "mean reward:200.000\n",
      "mean reward:200.000\n",
      "mean reward:167.900\n",
      "mean reward:143.340\n",
      "mean reward:143.770\n",
      "mean reward:199.890\n",
      "mean reward:198.270\n",
      "mean reward:169.160\n",
      "mean reward:182.940\n",
      "mean reward:171.590\n",
      "mean reward:200.000\n",
      "mean reward:200.000\n",
      "mean reward:200.000\n",
      "mean reward:200.000\n",
      "mean reward:107.870\n",
      "mean reward:20.020\n",
      "mean reward:98.270\n",
      "mean reward:10.090\n",
      "mean reward:67.560\n",
      "mean reward:196.270\n",
      "mean reward:196.930\n",
      "mean reward:190.240\n",
      "mean reward:186.470\n",
      "mean reward:151.360\n",
      "mean reward:176.390\n",
      "mean reward:194.760\n",
      "mean reward:200.000\n",
      "mean reward:200.000\n",
      "mean reward:200.000\n",
      "mean reward:200.000\n",
      "mean reward:200.000\n",
      "mean reward:200.000\n",
      "mean reward:200.000\n",
      "mean reward:191.130\n",
      "mean reward:196.350\n",
      "mean reward:186.000\n",
      "mean reward:92.810\n",
      "mean reward:172.680\n",
      "mean reward:200.000\n",
      "mean reward:200.000\n",
      "mean reward:191.230\n",
      "mean reward:196.360\n",
      "mean reward:185.470\n",
      "mean reward:194.830\n",
      "mean reward:198.140\n",
      "mean reward:198.150\n",
      "mean reward:180.630\n",
      "mean reward:154.840\n",
      "mean reward:142.590\n",
      "mean reward:134.240\n",
      "mean reward:83.230\n",
      "mean reward:191.460\n",
      "mean reward:198.260\n",
      "mean reward:200.000\n",
      "mean reward:191.050\n",
      "mean reward:200.000\n",
      "mean reward:183.640\n",
      "mean reward:187.640\n",
      "mean reward:92.810\n",
      "mean reward:88.230\n",
      "mean reward:138.670\n",
      "mean reward:155.770\n",
      "mean reward:200.000\n",
      "mean reward:200.000\n",
      "mean reward:200.000\n",
      "mean reward:200.000\n",
      "mean reward:196.070\n",
      "mean reward:181.010\n",
      "mean reward:152.950\n",
      "mean reward:102.290\n",
      "mean reward:104.730\n",
      "mean reward:104.260\n",
      "mean reward:98.840\n",
      "mean reward:103.290\n",
      "mean reward:112.370\n",
      "mean reward:116.210\n",
      "mean reward:130.130\n",
      "mean reward:178.340\n",
      "mean reward:191.480\n",
      "mean reward:167.620\n"
     ]
    }
   ],
   "source": [
    "s = tf.InteractiveSession()\n",
    "s.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(100):\n",
    "\n",
    "    rewards = [generate_session() for _ in range(100)]  # generate new sessions\n",
    "\n",
    "    print(\"mean reward:%.3f\" % (np.mean(rewards)))\n",
    "\n",
    "    if np.mean(rewards) > 300:\n",
    "        print(\"You Win!\") # but you can train even further\n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results & video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record sessions\n",
    "import gym.wrappers\n",
    "# env = gym.make(\"CartPole-v0\")\n",
    "env = gym.wrappers.Monitor(gym.make(\"CartPole-v0\"),\n",
    "                           directory=\"videos\", force=True)\n",
    "sessions = [generate_session() for _ in range(100)]\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(np.mean(sessions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"480\" controls>\n",
       "  <source src=\"./videos/openaigym.video.0.22.video000027.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show video\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(\n",
    "    filter(lambda s: s.endswith(\".mp4\"), os.listdir(\"./videos/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./videos/\"+video_names[-1]))  # this may or may not be _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted to Coursera platform. See results on assignment page!\n"
     ]
    }
   ],
   "source": [
    "from submit import submit_cartpole\n",
    "submit_cartpole(generate_session, 'dhinson.dacpano@ibm.com', 'h1N4xP8CwIB1bTbn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# That's all, thank you for your attention!\n",
    "# Not having enough? There's an actor-critic waiting for you in the honor section.\n",
    "# But make sure you've seen the videos first."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}